{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464b9b4a",
   "metadata": {},
   "source": [
    "# Section 5: Model Deployment - Training & Saving a Churn Prediction Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous session we trained a model for predicting churn and evaluated it. Now let's deploy it.\n",
    "\n",
    "**What is Model Deployment?**\n",
    "\n",
    "Deployment means taking a trained machine learning model and putting it into production where it can:\n",
    "1. **Accept new data** - Receive customer information in real-time\n",
    "2. **Make predictions** - Generate churn probability for new customers\n",
    "3. **Serve predictions** - Return results to business applications/users\n",
    "4. **Perform at scale** - Handle many requests efficiently\n",
    "\n",
    "**Why Save Models?**\n",
    "\n",
    "- **Reusability**: Train once, use many times without retraining\n",
    "- **Consistency**: Same model behavior across different environments\n",
    "- **Efficiency**: Don't retrain the entire pipeline each time\n",
    "- **Version control**: Keep track of which model is in production\n",
    "\n",
    "**This Notebook Covers:**\n",
    "\n",
    "1. **Data preparation** - Load and preprocess churn data\n",
    "2. **Model training** - Train logistic regression with cross-validation\n",
    "3. **Model persistence** - Save trained model to disk using pickle\n",
    "4. **Model loading** - Load saved model back into memory\n",
    "5. **Making predictions** - Use loaded model on new customer data\n",
    "6. **API integration** - Make HTTP requests to deployed model service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data processing, modeling, and evaluation\n",
    "\n",
    "import pandas as pd  # Data manipulation and loading\n",
    "import numpy as np   # Numerical computations\n",
    "\n",
    "# Model training and validation\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/test\n",
    "from sklearn.model_selection import KFold             # K-fold cross-validation\n",
    "\n",
    "# Feature engineering and model training\n",
    "from sklearn.feature_extraction import DictVectorizer  # Convert dicts to feature vectors\n",
    "from sklearn.linear_model import LogisticRegression   # Classification model\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import roc_auc_score  # Area under ROC curve metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the churn data\n",
    "df = pd.read_csv('data-week-3.csv')\n",
    "\n",
    "# Standardize column names: convert to lowercase and replace spaces with underscores\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Identify categorical (object dtype) columns\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "# Standardize categorical values: lowercase and replace spaces with underscores\n",
    "# This ensures consistency in feature names when vectorizing\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Convert totalcharges to numeric (some values might be empty/invalid)\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "# Fill missing values with 0 (customers with no previous charges)\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "# Convert churn target to binary: 'yes' -> 1, anything else -> 0\n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1903b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split data into training and test sets\n",
    "# We keep 20% of data for final testing (untouched during model development)\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4132a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define feature lists for modeling\n",
    "\n",
    "# Numerical features: continuous values that don't need encoding\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "# Categorical features: text values that need to be converted to numbers\n",
    "categorical = [\n",
    "    'gender',\n",
    "    'seniorcitizen',\n",
    "    'partner',\n",
    "    'dependents',\n",
    "    'phoneservice',\n",
    "    'multiplelines',\n",
    "    'internetservice',\n",
    "    'onlinesecurity',\n",
    "    'onlinebackup',\n",
    "    'deviceprotection',\n",
    "    'techsupport',\n",
    "    'streamingtv',\n",
    "    'streamingmovies',\n",
    "    'contract',\n",
    "    'paperlessbilling',\n",
    "    'paymentmethod',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92708443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define train() function - trains logistic regression model\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model on the given data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_train : DataFrame\n",
    "        Training data with features (categorical + numerical columns)\n",
    "    y_train : array-like\n",
    "        Binary target variable (0 or 1)\n",
    "    C : float\n",
    "        Regularization parameter (inverse of regularization strength)\n",
    "        Smaller C = stronger regularization (simpler model)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dv : DictVectorizer\n",
    "        Fitted vectorizer that transforms customer dicts to feature vectors\n",
    "    model : LogisticRegression\n",
    "        Trained logistic regression model\n",
    "    \"\"\"\n",
    "    # Convert DataFrame rows to list of dictionaries\n",
    "    # Each dict represents one customer's features\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    # Initialize DictVectorizer to convert categorical/numerical dicts to numeric arrays\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    # Fit vectorizer on training data and transform to feature matrix\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    # Train logistic regression model with regularization parameter C\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define predict() function - makes predictions on new data\n",
    "def predict(df, dv, model):\n",
    "    \"\"\"\n",
    "    Make churn probability predictions for new customers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Data to make predictions on (new customers)\n",
    "    dv : DictVectorizer\n",
    "        Fitted vectorizer (must be the same one used in training)\n",
    "    model : LogisticRegression\n",
    "        Trained model (must be trained with same dv)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : array\n",
    "        Predicted churn probability for each customer (0 to 1)\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to list of dictionaries (same format as training)\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    # Transform using the fitted vectorizer (no fitting, just transformation)\n",
    "    X = dv.transform(dicts)\n",
    "    \n",
    "    # Get probability predictions: model.predict_proba returns [prob_no_churn, prob_churn]\n",
    "    # We take [:, 1] to get probability of churn (second column)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Set hyperparameters for model training\n",
    "\n",
    "# C: Regularization strength parameter\n",
    "# C=1.0 is default; adjust based on cross-validation results\n",
    "C = 1.0\n",
    "\n",
    "# n_splits: Number of folds for k-fold cross-validation\n",
    "# 5-fold is standard; more folds = more reliable but slower\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce936aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.841 +- 0.008\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Perform k-fold cross-validation to estimate model performance\n",
    "\n",
    "# Initialize KFold splitter with shuffling for better estimation\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "# List to store AUC scores from each fold\n",
    "scores = []\n",
    "\n",
    "# Iterate through each fold (5 times in this case)\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    # Get training and validation data for this fold\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    # Extract target variable for this fold\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    # Train model on this fold's training data\n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    \n",
    "    # Make predictions on this fold's validation data\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    # Calculate ROC AUC score for this fold\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "# Print cross-validation results: mean AUC ± standard deviation\n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72b194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8423083263338855,\n",
       " 0.8450681201165409,\n",
       " 0.8324061810154525,\n",
       " 0.8319390707936304,\n",
       " 0.8522598914373568]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the individual AUC scores from each fold\n",
    "# Example output: [0.832, 0.848, 0.825, 0.851, 0.839]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e81326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572386167896259"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Final model training and test evaluation\n",
    "\n",
    "# Train final model on ALL training data (best practice after CV)\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "\n",
    "# Make predictions on held-out test set (never seen by model before)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "# Get test target values\n",
    "y_test = df_test.churn.values\n",
    "\n",
    "# Calculate ROC AUC on test set - this is our final performance metric\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc  # Display the final AUC score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847f586",
   "metadata": {},
   "source": [
    "## Section 5.1: Model Persistence - Saving Models to Disk\n",
    "\n",
    "**Why Save Models?**\n",
    "\n",
    "Instead of retraining the model every time we need predictions, we:\n",
    "1. **Train once** - Expensive operation (computationally)\n",
    "2. **Save to disk** - Serialization using pickle format (.bin files)\n",
    "3. **Load when needed** - Fast deserialization for serving predictions\n",
    "\n",
    "**Pickle Format:**\n",
    "\n",
    "- **What**: Python's native serialization format\n",
    "- **Pros**: Preserves all Python object structure (DictVectorizer + LogisticRegression)\n",
    "- **Cons**: Python-specific (not easily used in other languages)\n",
    "- **Use case**: Great for Python-based ML applications\n",
    "\n",
    "**Alternative Formats** (for production):\n",
    "- ONNX: Cross-language model format\n",
    "- Protocol Buffers: Google's serialization format\n",
    "- JSON: Human-readable but limited type support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a452f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle module for serializing Python objects\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d16a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output filename for saved model\n",
    "# Format: model_C=<C_value>.bin\n",
    "# The C value is included to track which hyperparameter was used\n",
    "output_file = f'model_C={C}.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533d28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the output filename\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old way: Manual file handling (less safe)\n",
    "# Open file in write-binary mode, dump model, close manually\n",
    "f_out = open(output_file, 'wb') \n",
    "pickle.dump((dv, model), f_out)\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx 1 alexey alexey 2.5K Sep 30 14:10 'model_C=1.0.bin'\r\n"
     ]
    }
   ],
   "source": [
    "# List binary files in current directory to verify model was saved\n",
    "# Shows filename and file size (-h for human-readable format)\n",
    "!ls -lh *.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better way: Use context manager (automatically closes file)\n",
    "# This is the recommended approach: safer and cleaner code\n",
    "with open(output_file, 'wb') as f_out: \n",
    "    pickle.dump((dv, model), f_out)\n",
    "# File is automatically closed when exiting the 'with' block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8a59e",
   "metadata": {},
   "source": [
    "## Section 5.2: Model Deserialization - Loading Models from Disk\n",
    "\n",
    "**Loading Saved Models:**\n",
    "\n",
    "Instead of retraining, we can load the previously saved model in a fraction of a second. This is how production systems work:\n",
    "1. Train model in development environment (notebook)\n",
    "2. Save to disk\n",
    "3. Deploy saved model to web server\n",
    "4. Load model at startup\n",
    "5. Serve predictions in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle for deserializing saved models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the saved model file\n",
    "# This is the file we saved earlier\n",
    "input_file = 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from disk\n",
    "# Use context manager to safely handle file operations\n",
    "with open(input_file, 'rb') as f_in: \n",
    "    # pickle.load deserializes the tuple: (dv, model)\n",
    "    dv, model = pickle.load(f_in)\n",
    "# Now we have dv and model ready to use for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806ef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the loaded model object\n",
    "# Shows it's a LogisticRegression model with C=1.0 parameter\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d96d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded model with a single customer record\n",
    "# This is a sample customer with all required features\n",
    "\n",
    "customer = {\n",
    "    'gender': 'female',                    # Customer demographic\n",
    "    'seniorcitizen': 0,                    # Age group (0 = not senior)\n",
    "    'partner': 'yes',                      # Has a partner\n",
    "    'dependents': 'no',                    # No dependents\n",
    "    'phoneservice': 'no',                  # Phone service status\n",
    "    'multiplelines': 'no_phone_service',   # Multiple lines\n",
    "    'internetservice': 'dsl',              # Internet type\n",
    "    'onlinesecurity': 'no',                # Online security add-on\n",
    "    'onlinebackup': 'yes',                 # Online backup add-on\n",
    "    'deviceprotection': 'no',              # Device protection\n",
    "    'techsupport': 'no',                   # Tech support\n",
    "    'streamingtv': 'no',                   # TV streaming service\n",
    "    'streamingmovies': 'no',               # Movie streaming service\n",
    "    'contract': 'month-to-month',          # Contract type (high churn risk)\n",
    "    'paperlessbilling': 'yes',             # Paperless billing enabled\n",
    "    'paymentmethod': 'electronic_check',   # Payment method\n",
    "    'tenure': 1,                           # Months as customer (NEW customer!)\n",
    "    'monthlycharges': 29.85,               # Monthly bill\n",
    "    'totalcharges': 29.85                  # Total charges (just first month)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform customer dictionary to feature vector using the loaded DictVectorizer\n",
    "# This converts categorical variables to numeric one-hot encoded features\n",
    "X = dv.transform([customer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0758291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using the loaded model\n",
    "# predict_proba returns probabilities for both classes: [no_churn, churn]\n",
    "# [0, 1] gets the churn probability for first (and only) customer\n",
    "y_pred = model.predict_proba(X)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7d951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: {'gender': 'female', 'seniorcitizen': 0, 'partner': 'yes', 'dependents': 'no', 'phoneservice': 'no', 'multiplelines': 'no_phone_service', 'internetservice': 'dsl', 'onlinesecurity': 'no', 'onlinebackup': 'yes', 'deviceprotection': 'no', 'techsupport': 'no', 'streamingtv': 'no', 'streamingmovies': 'no', 'contract': 'month-to-month', 'paperlessbilling': 'yes', 'paymentmethod': 'electronic_check', 'tenure': 1, 'monthlycharges': 29.85, 'totalcharges': 29.85}\n",
      "output: 0.5912433520805763\n"
     ]
    }
   ],
   "source": [
    "# Display input customer data and output prediction\n",
    "# y_pred is probability between 0 and 1\n",
    "# 0.8 means 80% chance customer will churn, 20% chance they'll stay\n",
    "print('input:', customer)\n",
    "print('output:', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bfa7d",
   "metadata": {},
   "source": [
    "## Section 5.3: Production Deployment - API Integration\n",
    "\n",
    "**From Notebook to Web Service:**\n",
    "\n",
    "So far we've tested predictions locally. In production:\n",
    "1. **Web Server** - Receives HTTP requests from client applications\n",
    "2. **Model Service** - Endpoint that accepts customer data (JSON)\n",
    "3. **Prediction** - Model processes data and returns churn probability\n",
    "4. **Response** - Web service returns prediction back to client (JSON)\n",
    "\n",
    "**Why Use HTTP/API?**\n",
    "- **Language-agnostic**: Clients can use any language (Python, JavaScript, Java, etc.)\n",
    "- **Scalable**: Can handle multiple requests concurrently\n",
    "- **Stateless**: Each request is independent\n",
    "- **Deployable**: Can run on any cloud platform (AWS, GCP, Azure, Heroku)\n",
    "\n",
    "**Example Flow:**\n",
    "```\n",
    "Client App (e.g., CRM System)\n",
    "    ↓\n",
    "HTTP POST /predict\n",
    "    ↓\n",
    "Web Server (Flask/FastAPI/Django)\n",
    "    ↓\n",
    "Load model from disk\n",
    "    ↓\n",
    "Transform customer data\n",
    "    ↓\n",
    "Get prediction\n",
    "    ↓\n",
    "HTTP Response (JSON)\n",
    "    ↓\n",
    "Client receives churn probability\n",
    "    ↓\n",
    "Send automated email to at-risk customers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests library to make HTTP calls to web server\n",
    "# This is how client applications would interact with deployed model\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the deployed prediction service\n",
    "# In production, this would point to your web server (local or cloud-based)\n",
    "url = 'http://localhost:9696/predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another test customer record to send to the API\n",
    "# This customer is different from the first: has 2-year contract (lower churn risk)\n",
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'two_year',  # 2-year contract (more committed customer)\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send customer data to the web service as JSON and get prediction\n",
    "# requests.post sends HTTP POST request with customer data\n",
    "# .json() parses the JSON response from server\n",
    "response = requests.post(url, json=customer).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'churn': True, 'churn_probability': 0.5133820686195286}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the response from web service\n",
    "# Expected format: {'churn': True/False, 'churn_probability': 0.XXXX}\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db38caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending email to asdx-123d\n"
     ]
    }
   ],
   "source": [
    "# Business logic: If prediction shows high churn probability, take action\n",
    "# Example: Send retention email or special offer\n",
    "if response['churn']:\n",
    "    # In production, this would integrate with CRM/email system\n",
    "    print('sending email to', 'asdx-123d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9a70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
